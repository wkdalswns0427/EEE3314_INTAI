{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"장민준_2019145094_Assignment2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"NZZzm8PY2gNR"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn.datasets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rjuQY9f2mdS"},"source":["## Assignment 2-1\n","ReLu activation function을 구현해보세요\n","\n","- Hint : np.maximum 함수 사용하면 편리합니다\n","- 다른 방법 사용하셔도 무방합니다\n"]},{"cell_type":"code","metadata":{"id":"puH0YVGI2uLz"},"source":["def relu(x):\n","  R = np.maximum(0,x)\n","  return R"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wz8Hi0Rc2-yJ"},"source":["##Assignment 2-2\n","ReLu의 derivative function을 구현해보세요\n"]},{"cell_type":"code","metadata":{"id":"fusEy49j3uhs"},"source":["def d_relu(x):\n","  x[x<=0]=0\n","  x[x>0]=1\n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twf-R8s-34zT"},"source":["## Assignment 2-3\n","Lecture 2의 2. Backpropagation with numpy 코드 참고해서\n","Three layer MLP를 구한후, 학습을 돌려 보세요\n","\n","hyperparameter는 다음과 같이 설정\n","\n","- <#> of train data, <#> of test data : 60000, 10000\n","- epochs : 100\n","- hiddensize : 128, 64 (two layer)\n","- learning_rate : 0.5"]},{"cell_type":"code","metadata":{"id":"bxJO249A3jhk"},"source":["# Assignment 2-3 구현은 여기서 ()\n","from IPython import get_ipython\n","get_ipython().magic('reset -sf')\n","import numpy as np\n","import sklearn.datasets\n","\n","mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wSZPn0ql_bCk"},"source":["num_train = 60000\n","num_class = 10\n","\n","x_train = np.float32(mnist.data[:num_train]).T\n","y_train_index = np.int32(mnist.target[:num_train]).T\n","x_test = np.float32(mnist.data[num_train:]).T\n","y_test_index = np.int32(mnist.target[num_train:]).T\n","\n","# Normalization\n","\n","x_train /= 255\n","x_test /= 255\n","x_size = x_train.shape[0]\n","\n","y_train = np.zeros((num_class, y_train_index.shape[0]))\n","for idx in range(y_train_index.shape[0]):\n","  y_train[y_train_index[idx], idx] = 1\n","\n","y_test = np.zeros((num_class, y_test_index.shape[0]))\n","for idx in range(y_test_index.shape[0]):\n","  y_test[y_test_index[idx], idx] = 1  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5di-0BDC_pGX"},"source":["hidden_size_1 = 128 \n","hidden_size_2 = 64\n","\n","# three-layer neural network\n","params = {\"W1\": np.random.randn(hidden_size_1, x_size) * np.sqrt(1/ x_size),\n","          \"b1\": np.zeros((hidden_size_1, 1)) * np.sqrt(1/ x_size),\n","          \"W2\": np.random.randn(hidden_size_2, hidden_size_1) * np.sqrt(1/ hidden_size_1),\n","          \"b2\": np.zeros((hidden_size_2, 1)) * np.sqrt(1/ hidden_size_1),\n","          \"W3\": np.random.randn(num_class, hidden_size_2) * np.sqrt(1/ hidden_size_2),\n","          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size_2)\n","          }\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAWPAIZrDtj9"},"source":["def sigmoid(x):\n","  return 1/(1+np.exp(-x))\n","\n","def d_sigmoid(x):\n","  # derivative of sigmoid\n","  exp = np.exp(-x)\n","  return (exp)/((1+exp)**2)\n","\n","def softmax(x):\n","  exp = np.exp(x)\n","  return exp/np.sum(exp, axis=0)\n","\n","def compute_loss(y_true, y_pred):\n","  # loss calculation\n","  num_sample = y_true.shape[1]\n","  Li = -1 * np.sum(y_true * np.log(y_pred))\n","  \n","  return Li/num_sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGTp1511DtgV"},"source":["def foward_pass(x, params):\n","  \n","  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params[\"A1\"] = sigmoid(params[\"S1\"])\n","  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n","  params[\"A2\"] = sigmoid(params[\"S2\"])\n","  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n","  params[\"A3\"] = softmax(params[\"S3\"])\n","\n","  return params"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bKNrLEJVDtca"},"source":["def foward_pass_test(x, params):\n","\n","  params_test = {}\n","  \n","  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params_test[\"A1\"] = sigmoid(params_test[\"S1\"])\n","  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n","  params_test[\"A2\"] = sigmoid(params_test[\"S2\"])\n","  params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n","  params_test[\"A3\"] = softmax(params_test[\"S3\"])\n","\n","  return params_test\n","\n","def compute_accuracy(y_true, y_pred):\n","  y_true_idx = np.argmax(y_true, axis = 0)\n","  y_pred_idx = np.argmax(y_pred, axis = 0)\n","  num_correct = np.sum(y_true_idx==y_pred_idx)\n","\n","  accuracy = num_correct / y_true.shape[1] * 100\n","\n","  return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ydl4PPmKDtMT"},"source":["def backward_pass(x, y_true, params):\n","\n","  dS3 = params[\"A3\"] - y_true\n","\n","  grads = {}\n","\n","  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n","  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA2 = np.dot(params[\"W3\"].T, dS3)\n","  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n","\n","  grads[\"dW2\"] = np.dot(dS2,  params[\"A1\"].T)/x.shape[1]\n","  grads[\"db2\"] = np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA1 = np.dot(params[\"W2\"].T, dS2)\n","  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n","\n","  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n","  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n","\n","  return grads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"adO7stshDtIP","executionInfo":{"status":"ok","timestamp":1632632180205,"user_tz":-540,"elapsed":221435,"user":{"displayName":"‍장민준(학부학생/공과대학 기계공학)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03438421046769211824"}},"outputId":"21fbf170-987b-4489-d702-45737b22165f"},"source":["epochs = 100\n","learning_rate = 0.5\n","\n","for i in range(1,epochs+1):\n","\n","  if i == 1:\n","    params = foward_pass(x_train, params)\n","    \n","  grads = backward_pass(x_train, y_train, params)\n","\n","  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n","  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n","  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n","  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n","  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n","  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n","\n","  params = foward_pass(x_train, params)\n","  train_loss = compute_loss(y_train, params[\"A3\"])\n","  train_acc = compute_accuracy(y_train, params[\"A3\"])\n","\n","  params_test = foward_pass_test(x_test, params)\n","  test_loss = compute_loss(y_test, params_test[\"A3\"])\n","  test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n","  if i % 10 == 0:\n","    print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n","    .format(i, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 10: training loss = 2.274707, training acuracy = 12.12%, test loss = 2.273789, training acuracy = 12.38%\n","Epoch 20: training loss = 2.245321, training acuracy = 25.97%, test loss = 2.243568, training acuracy = 26.93%\n","Epoch 30: training loss = 2.204654, training acuracy = 41.5%, test loss = 2.201735, training acuracy = 43.16%\n","Epoch 40: training loss = 2.143366, training acuracy = 49.86%, test loss = 2.13871, training acuracy = 51.35%\n","Epoch 50: training loss = 2.049696, training acuracy = 53.76%, test loss = 2.042515, training acuracy = 54.51%\n","Epoch 60: training loss = 1.916781, training acuracy = 56.42%, test loss = 1.906461, training acuracy = 56.86%\n","Epoch 70: training loss = 1.754021, training acuracy = 59.48%, test loss = 1.740753, training acuracy = 59.56%\n","Epoch 80: training loss = 1.581855, training acuracy = 62.52%, test loss = 1.566607, training acuracy = 62.95%\n","Epoch 90: training loss = 1.418251, training acuracy = 65.44%, test loss = 1.402213, training acuracy = 65.97%\n","Epoch 100: training loss = 1.273756, training acuracy = 67.95%, test loss = 1.257844, training acuracy = 68.19%\n"]}]},{"cell_type":"markdown","metadata":{"id":"WaqqRzF73oBu"},"source":["## Assignment 2-4\n","Lecture 2의 2. backpropagatin with numpy 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n","\n","- Hint : Activation function, hyperparameter setting"]},{"cell_type":"code","metadata":{"id":"PiIb-hNkDgY-","executionInfo":{"status":"ok","timestamp":1632642562833,"user_tz":-540,"elapsed":23020,"user":{"displayName":"‍장민준(학부학생/공과대학 기계공학)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03438421046769211824"}}},"source":["# Assignment 2-4 구현은 여기서 ()\n","from IPython import get_ipython\n","get_ipython().magic('reset -sf')\n","\n","import numpy as np\n","import sklearn.datasets\n","\n","mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")\n","\n","num_train = 60000\n","num_class = 10\n","\n","x_train = np.float32(mnist.data[:num_train]).T\n","y_train_index = np.int32(mnist.target[:num_train]).T\n","x_test = np.float32(mnist.data[num_train:]).T\n","y_test_index = np.int32(mnist.target[num_train:]).T\n","\n","# Normalization\n","\n","x_train /= 255\n","x_test /= 255\n","x_size = x_train.shape[0]\n","\n","y_train = np.zeros((num_class, y_train_index.shape[0]))\n","for idx in range(y_train_index.shape[0]):\n","  y_train[y_train_index[idx], idx] = 1\n","\n","y_test = np.zeros((num_class, y_test_index.shape[0]))\n","for idx in range(y_test_index.shape[0]):\n","  y_test[y_test_index[idx], idx] = 1    "],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"q75VLgQmGLMV","executionInfo":{"status":"ok","timestamp":1632643290197,"user_tz":-540,"elapsed":1005,"user":{"displayName":"‍장민준(학부학생/공과대학 기계공학)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03438421046769211824"}}},"source":["hidden_size_1 = 128 \n","hidden_size_2 = 64\n","hidden_size_3 = 64\n","hidden_size_4 = 32\n","\n","# five-layer neural network\n","params = {\"W1\": np.random.randn(hidden_size_1, x_size) * np.sqrt(1/ x_size),\n","          \"b1\": np.zeros((hidden_size_1, 1)) * np.sqrt(1/ x_size),\n","\n","          \"W2\": np.random.randn(hidden_size_2, hidden_size_1) * np.sqrt(1/ hidden_size_1),\n","          \"b2\": np.zeros((hidden_size_2, 1)) * np.sqrt(1/ hidden_size_1),\n","\n","          \"W3\": np.random.randn(hidden_size_3, hidden_size_2) * np.sqrt(1/ hidden_size_2),\n","          \"b3\": np.zeros((hidden_size_3, 1)) * np.sqrt(1/ hidden_size_2),\n","\n","          \"W4\": np.random.randn(hidden_size_4, hidden_size_3) * np.sqrt(1/ hidden_size_3),\n","          \"b4\": np.zeros((hidden_size_4, 1)) * np.sqrt(1/ hidden_size_3),\n","\n","          \"W5\": np.random.randn(num_class, hidden_size_4) * np.sqrt(1/ hidden_size_4),\n","          \"b5\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size_4),\n","          \n","          }"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIe--a4zGR2X","executionInfo":{"status":"ok","timestamp":1632643290199,"user_tz":-540,"elapsed":5,"user":{"displayName":"‍장민준(학부학생/공과대학 기계공학)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03438421046769211824"}}},"source":["def relu(x):\n","  R = np.maximum(0,x)\n","  return R\n","\n","def d_relu(x):\n","  x[x<=0]=0\n","  x[x>0]=1\n","  return x\n","\n","def softmax(x):\n","  exp = np.exp(x)\n","  return exp/np.sum(exp, axis=0)\n","\n","def compute_loss(y_true, y_pred):\n","  # loss calculation\n","  num_sample = y_true.shape[1]\n","  Li = -1 * np.sum(y_true * np.log(y_pred))\n","  \n","  return Li/num_sample"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"LA8QuNPXGacQ","executionInfo":{"status":"ok","timestamp":1632643290809,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍장민준(학부학생/공과대학 기계공학)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03438421046769211824"}}},"source":["def foward_pass(x, params):\n","  \n","  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params[\"A1\"] = relu(params[\"S1\"])\n","  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n","  params[\"A2\"] = relu(params[\"S2\"])\n","  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n","  params[\"A3\"] = relu(params[\"S3\"])\n","  params[\"S4\"] = np.dot(params[\"W4\"], params[\"A3\"]) + params[\"b4\"]\n","  params[\"A4\"] = relu(params[\"S4\"])\n","  params[\"S5\"] = np.dot(params[\"W5\"], params[\"A4\"]) + params[\"b5\"]\n","  params[\"A5\"] = softmax(params[\"S5\"])\n","\n","  return params\n","\n","def foward_pass_test(x, params):\n","\n","  params_test = {}\n","  \n","  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params_test[\"A1\"] = relu(params_test[\"S1\"])\n","  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n","  params_test[\"A2\"] = relu(params_test[\"S2\"])\n","  params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n","  params_test[\"A3\"] = relu(params_test[\"S3\"])\n","  params_test[\"S4\"] = np.dot(params[\"W4\"], params_test[\"A3\"]) + params[\"b4\"]\n","  params_test[\"A4\"] = relu(params_test[\"S4\"])\n","  params_test[\"S5\"] = np.dot(params[\"W5\"], params_test[\"A4\"]) + params[\"b5\"]\n","  params_test[\"A5\"] = softmax(params_test[\"S5\"])\n","\n","  return params_test\n","\n","def compute_accuracy(y_true, y_pred):\n","  y_true_idx = np.argmax(y_true, axis = 0)\n","  y_pred_idx = np.argmax(y_pred, axis = 0)\n","  num_correct = np.sum(y_true_idx==y_pred_idx)\n","\n","  accuracy = num_correct / y_true.shape[1] * 100\n","\n","  return accuracy"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTLzfvqkGgRf","executionInfo":{"status":"ok","timestamp":1632643291312,"user_tz":-540,"elapsed":11,"user":{"displayName":"‍장민준(학부학생/공과대학 기계공학)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03438421046769211824"}}},"source":["def backward_pass(x, y_true, params):\n","\n","  dS5 = params[\"A5\"] - y_true\n","\n","  grads = {}\n","\n","  grads[\"dW5\"] =  np.dot(dS5, params[\"A4\"].T)/x.shape[1]\n","  grads[\"db5\"] =  (1/x.shape[1])*np.sum(dS5, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA4 = np.dot(params[\"W5\"].T, dS5)\n","  dS4 = dA4 * d_relu(params[\"S4\"])\n","\n","  grads[\"dW4\"] = np.dot(dS4,  params[\"A3\"].T)/x.shape[1]\n","  grads[\"db4\"] = np.sum(dS4, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA3 = np.dot(params[\"W4\"].T, dS4)\n","  dS3 = dA3 * d_relu(params[\"S3\"])\n","\n","  grads[\"dW3\"] = np.dot(dS3,  params[\"A2\"].T)/x.shape[1]\n","  grads[\"db3\"] = np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA2 = np.dot(params[\"W3\"].T, dS3)\n","  dS2 = dA2 * d_relu(params[\"S2\"])\n","\n","  grads[\"dW2\"] = np.dot(dS2,  params[\"A1\"].T)/x.shape[1]\n","  grads[\"db2\"] = np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA1 = np.dot(params[\"W2\"].T, dS2)\n","  dS1 = dA1 * d_relu(params[\"S1\"])\n","\n","  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n","  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n","\n","  return grads"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8odB0SEGmm6","executionInfo":{"status":"ok","timestamp":1632645142932,"user_tz":-540,"elapsed":1851627,"user":{"displayName":"‍장민준(학부학생/공과대학 기계공학)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03438421046769211824"}},"outputId":"e30770d5-2c90-4bc1-b11f-f3d63e7a7d99"},"source":["epochs = 1000\n","learning_rate = 0.05\n","\n","for i in range(0,epochs+1):\n","\n","  if i == 0:\n","    params = foward_pass(x_train, params)\n","    \n","  grads = backward_pass(x_train, y_train, params)\n","\n","  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n","  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n","  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n","  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n","  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n","  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n","  params[\"W4\"] -= learning_rate * grads[\"dW4\"]\n","  params[\"b4\"] -= learning_rate * grads[\"db4\"]\n","  params[\"W5\"] -= learning_rate * grads[\"dW5\"]\n","  params[\"b5\"] -= learning_rate * grads[\"db5\"]\n","\n","  params = foward_pass(x_train, params)\n","  train_loss = compute_loss(y_train, params[\"A5\"])\n","  train_acc = compute_accuracy(y_train, params[\"A5\"])\n","\n","  params_test = foward_pass_test(x_test, params)\n","  test_loss = compute_loss(y_test, params_test[\"A5\"])\n","  test_acc = compute_accuracy(y_test, params_test[\"A5\"])\n","  if i % 100 == 0 :\n","    print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n","    .format(i, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: training loss = 2.306689, training acuracy = 9.01%, test loss = 2.307064, training acuracy = 9.11%\n","Epoch 100: training loss = 1.531997, training acuracy = 58.6%, test loss = 1.517695, training acuracy = 59.04%\n","Epoch 200: training loss = 0.568771, training acuracy = 84.33%, test loss = 0.551862, training acuracy = 84.75%\n","Epoch 300: training loss = 0.400079, training acuracy = 88.68%, test loss = 0.387023, training acuracy = 89.13%\n","Epoch 400: training loss = 0.33278, training acuracy = 90.5%, test loss = 0.3219, training acuracy = 90.65%\n","Epoch 500: training loss = 0.286499, training acuracy = 91.81%, test loss = 0.277308, training acuracy = 92.0%\n","Epoch 600: training loss = 0.259458, training acuracy = 92.48%, test loss = 0.251373, training acuracy = 92.79%\n","Epoch 700: training loss = 0.234347, training acuracy = 93.28%, test loss = 0.228377, training acuracy = 93.49%\n","Epoch 800: training loss = 0.214651, training acuracy = 93.86%, test loss = 0.210628, training acuracy = 93.91%\n","Epoch 900: training loss = 0.198208, training acuracy = 94.32%, test loss = 0.196148, training acuracy = 94.32%\n","Epoch 1000: training loss = 0.183131, training acuracy = 94.79%, test loss = 0.18307, training acuracy = 94.58%\n"]}]},{"cell_type":"markdown","metadata":{"id":"pboMIBQq7onH"},"source":["**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"]},{"cell_type":"markdown","metadata":{"id":"bgFqABVfefF1"},"source":["\n","\n","```\n","# 코드로 형식 지정됨\n","```\n","\n","1. 2 layer MLP를 5 layer MLP로 신경망을 다층화함\n","- 보다 깊은 신경망을 거쳐 정교한 결과를 유도한다.\n","\n","2. training epoch를 1000회로 증가\n","- 더 많은 횟수의 학습으로 정확도를 상승시킨다.\n","\n","3. learning rate를 0.05로 줄여 정교화\n","- 경험을 통해 0.1의 learning rate이 가장 이상적임을 확인하였다.\n","\n","4. activation function을 sigmoid에서 relu로 변화\n","- input이 0~255 사이의 데이터로 sigmoid function의 범위를 넘어가 많은 데이터가 0,1로 수렴하는 경우가 발생하고 gradient가 0으로 수렴하게 되어 relu에 비해 많은 cost가 발생한다."]}]}